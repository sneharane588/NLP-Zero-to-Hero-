{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1_Tokenization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGONc0XV_FGl"
      },
      "source": [
        "# Tokenization\n",
        "\n",
        "Represent words in a way that computer can process them with a view,<br>\n",
        "Then training them with a Neural networds that can understand their meaning.<br/>\n",
        "![](https://miro.medium.com/max/2414/1*UhfwmhMN9sdfcWIbO5_tGg.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b20-GRZu6Dji"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmCmHBTg6ZYK"
      },
      "source": [
        "# python array of strings\n",
        "\n",
        "sentences=[\n",
        "           'I love my Dog',\n",
        "           'I love my Cat',\n",
        "           'i love my Dog!'            \n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybl1c5SX6uBw"
      },
      "source": [
        "# create an instance of Tokenizer object\n",
        "# num_words - max number of words to keep\n",
        "# i.e. we just want to keep top 100 frequent words from thousands of books\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyW4tKH_6yRx"
      },
      "source": [
        "# go through all the texts anf fit the tokeniser onto it\n",
        "\n",
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v484sZs68wy5"
      },
      "source": [
        "Properties of Tokenizer object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQtgS5jt63nc",
        "outputId": "006e1eeb-20f2-4b92-e56f-42dd7e229597"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(word_index)  # will  return a dictionary"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvNa3UbM7Dya",
        "outputId": "78db288d-15cb-42de-be83-f38f50333ab5"
      },
      "source": [
        "word_counts = tokenizer.word_counts\n",
        "print(word_counts)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OrderedDict([('i', 3), ('love', 3), ('my', 3), ('dog', 2), ('cat', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Kl7fTk7MPV",
        "outputId": "8f7d1fc0-27be-4a32-91bc-580b032a3bcd"
      },
      "source": [
        "word_docs = tokenizer.word_docs\n",
        "print(word_docs)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'int'>, {'i': 3, 'my': 3, 'dog': 2, 'love': 3, 'cat': 1})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in1aQOlN7SLy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f333a9cb-f56f-4bf1-e3e3-69de6f62d851"
      },
      "source": [
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "token = Tokenizer(num_words = 100)\n",
        "token.fit_on_texts(sentences)\n",
        "word_index = token.word_index\n",
        "print(word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xacbCJWH-a4T"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}