{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP2_Sequencing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euMj24G7CXM6"
      },
      "source": [
        "# Sequencing - Turning sentences into data\n",
        "![](https://miro.medium.com/max/1218/1*zsIXWoN0_CE9PXzmY3tIjQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89AbHEnxCT3r"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nJo05iVCYk5"
      },
      "source": [
        "# python array of strings\n",
        "\n",
        "sentences=[\n",
        "           'I love my Dog',\n",
        "           'i love my Cat',\n",
        "           'You love my Dog!',\n",
        "           'Do you think my Dog is amazing!'            \n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXzytJ99DY6y"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pW6ua0UjEDf2"
      },
      "source": [
        "## texts_to_sequences()\n",
        "Creates sequences of tokens representing each sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI8DpUi7Dr_Z"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2LV06lrDoA5",
        "outputId": "cf75d0c8-9f46-40a6-8a2b-721d7dcf9f26"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb1NUKMkD5GK",
        "outputId": "7e91cbb2-6130-4513-f57b-3b7f5cd663f8"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbDt-ssAEU25"
      },
      "source": [
        "This for getting data ready for Neural network,\n",
        "### but how to handle this when NN needs to classify text but there are words in the text that are never seen before....."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipn_WTu0Fx8i",
        "outputId": "36b66a31-08ab-4d82-9a22-92f8d9461541"
      },
      "source": [
        "test_data = [\n",
        "             'i really love my dog', # 5 words sentence\n",
        "             'my dog loves my mantee'\n",
        "]\n",
        "\n",
        "# applied texts_to_sequences() method without fit_on texts()\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)  # indexes not present in word_index are skipped"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzC7WyEXHO9g"
      },
      "source": [
        "Here we lost the length ofthe sequence,<br>\n",
        "to avoid this,<br>\n",
        "use <b>oov_token</b> property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVgyikHOIm5T"
      },
      "source": [
        "arr =[\n",
        "           'I love my Dog',\n",
        "           'i love my Cat',\n",
        "           'You love my Dog!',\n",
        "           'Do you think my Dog is amazing!'            \n",
        "]\n",
        "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(arr)\n",
        "wi = tokenizer.word_index\n",
        "seq = tokenizer.texts_to_sequences(arr)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6n0ePiqKFvK",
        "outputId": "df37ff28-d048-411c-abfa-b5bebb8c3638"
      },
      "source": [
        "print(wi)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4eJS9VIJiUH",
        "outputId": "8c9d824f-3ada-49f3-fffc-77014181704f"
      },
      "source": [
        "test_arr = [\n",
        "             'i really love my dog', # 5 words sentence\n",
        "             'my dog loves my mantee'\n",
        "]\n",
        "\n",
        "# applied texts_to_sequences() method without fit_on texts()\n",
        "test_seq = tokenizer.texts_to_sequences(test_arr)\n",
        "print(test_seq)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3ZWLzgoJ9xi",
        "outputId": "edcc581a-797b-479e-f0a1-828adaa05cfe"
      },
      "source": [
        "print(wi)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k958Fj8rKnfl",
        "outputId": "7d3eadce-7f24-41d9-a25b-6533c4dc2f6e"
      },
      "source": [
        "print(wi['<OOV>'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJcaAqqiKt-k"
      },
      "source": [
        "Here we still lost some meaning but a lot less, but we've maintained length of the sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZwSQENMLYVq"
      },
      "source": [
        "But how we can handle sentences of different length when training a NN<br>Advanced Solution is <b>RaggedTensor</b><br>but Simple solution is <b>Padding</b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtVORV-FNcIt"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnwaDKmHMRrf"
      },
      "source": [
        "arr1 =[\n",
        "           'I love my Dog',\n",
        "           'i love my Cat',\n",
        "           'You love my Dog!',\n",
        "           'Do you think my Dog is amazing!'            \n",
        "]\n",
        "tokenizer1 = Tokenizer(num_words=100, oov_token=\"<OOV>\")\n",
        "tokenizer1.fit_on_texts(arr1)\n",
        "wi1 = tokenizer1.word_index\n",
        "seq1 = tokenizer1.texts_to_sequences(arr1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYMifiWyNQMS"
      },
      "source": [
        "# measures length of longest sentence and ensures sequence of all he sentences have equal length by adding padding of zeros to it\n",
        "padded = pad_sequences(seq1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJZEWEaHNZ56",
        "outputId": "a43e1565-5238-4bdf-a39c-f32b8a11284e"
      },
      "source": [
        "print(wi1)\n",
        "print(seq1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMERKg_1N0l7",
        "outputId": "5a8f8137-b8e6-40ee-bbb3-3c467b735613"
      },
      "source": [
        "print(padded)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQHCDl_LN2kO",
        "outputId": "fa5aaa61-6ad7-44ad-db3d-1d4cd80ccdec"
      },
      "source": [
        "padded = pad_sequences(seq1, padding='post') # if you want zeros after the sequence\n",
        "padded"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5,  3,  2,  4,  0,  0,  0],\n",
              "       [ 5,  3,  2,  7,  0,  0,  0],\n",
              "       [ 6,  3,  2,  4,  0,  0,  0],\n",
              "       [ 8,  6,  9,  2,  4, 10, 11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEDqh0qQPU2U",
        "outputId": "36c74d76-85a6-40b0-93d3-d7cef2a58bb9"
      },
      "source": [
        "padded = pad_sequences(seq1, maxlen=5, truncating='post') # keep only 5 digit sequence and chop sequence from last(post-truncation)\n",
        "                                                          # by default is pre-truncation\n",
        "padded"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 5, 3, 2, 4],\n",
              "       [0, 5, 3, 2, 7],\n",
              "       [0, 6, 3, 2, 4],\n",
              "       [8, 6, 9, 2, 4]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJh-xavkPmKV"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}